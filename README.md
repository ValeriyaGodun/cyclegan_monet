# CycleGAN: Перенос стиля Клода Моне

## Описание задачи

В данном проекте показана реализация CycleGAN для переноса художественного стиля на фотографии. Проект выполнен в рамках Kaggle соревнования "I'm Something of a Painter Myself": https://www.kaggle.com/competitions/gan-getting-started/overview.

Цель - преобразовать обычные фотографии в картины в стиле Клода Моне.

В соревновании Kaggle "I'm Something of a Painter Myself" данная реализация CycleGAN входит в топ 15%  решений. 

## Архитектура

CycleGAN использует две пары моделей:

**Два генератора:**
- `G_photo_to_monet` - преобразует фотографии в картины стиля Моне
- `G_monet_to_photo` - выполняет обратное преобразование

**Два дискриминатора:**
- `D_photo` - различает реальные и сгенерированные фотографии
- `D_monet` - различает реальные картины Моне и сгенерированные

## Датасет

Для обучения использовались данные из Kaggle competition, состоящие из двух независимых частей:

- Картины Моне:
300 оцифрованных картин Клода Моне.
!!!здесь добавить пример картин

- Фотографии:
7028 современных различных фотографий.
!!!здесь добавить пример фото

Для оптимизации времени обучения (из-за ограничений в соревновании) было случайным образом отобрано 1000 изображений.

*Ключевая особенность датасета - непарность*
В отличие от классических задач image-to-image, здесь отсутствуют парные данные. Это означает, что нет конкретных пар "фотография - соответствующая ей картина Моне". Это сложнее, так как в парных данных модель просто учится отображению (фото собаки - картина этой же собаки в стиле Моне), а в непарных данных модель должна понять абстрактное понятие "стиль Моне" и применить его к любой фотографии.
Решение данной проблемы заключается в использовании cycle consistency loss - функция потерь, которая обеспечивает циклическую согласованность - преобразованное изображение можно обратно восстановить в исходное, сохранив структуру и ключевые элементы сцены.Это заменяет необходимость в парных данных и позволяет модели учиться переносу стиля без явного соответствия между изображениями.

## Функция потерь

Общая функция потерь генератора состоит из трех компонентов:

1. **Adversarial Loss** (MSELoss)
   - Обманывает дискриминаторы

2. **Cycle Consistency Loss** (L1Loss, λ=10.0)
   - Гарантирует обратимость преобразования

3. **Identity Loss** (L1Loss, λ=0.5)
   - Сохраняет цветовую схему и предотвращает излишние искажения цветов

**Итоговая формула:**
Total_Loss = Adversarial_Loss + 10.0 * Cycle_Loss + 5.0 * Identity_Loss,
где Identity_Loss умножается на λ_cycle * λ_identity = 10.0 * 0.5 = 5.0


- **Image Pool:** Буфер из 50 изображений для обучения дискриминаторов
- **Learning Rate Scheduling:** StepLR (снижение в 2 раза каждые 50 эпох)
- **Residual Connections:** Skip connections для глубоких сетей
- **InstanceNorm:** Стабильнее BatchNorm для style transfer

## Техники стабилизации

GAN известны нестабильностью обучения. Для решения нестабильности обучения были применены следующие техники:

### Image Pool
 Дискриминатор, обучаясь только на самых свежих сгенерированных изображениях, может "забывать" предыдущие и начинать колебаться. Image Pool в данном проекте хранит историю из 50 последних сгенерированных изображений. При обучении дискриминатора с вероятностью 50% используется случайное изображение из пула вместо текущего. Это заставляет дискриминатор помнить о разных вариантах генерации и делает обучение более стабильным.

### Learning Rate Scheduling
 В начале обучения нужен высокий learning rate для быстрого прогресса. По мере приближения к оптимуму, слишком большой шаг может привести к "проскакиванию" минимума. Постепенное снижение learning rate позволяет модели "устояться" и найти более качественное решение.

 ### Residual Connections
При прохождении через множество слоев возникает проблема затухающих градиентов. Поэтому было решено добавить residual блоки со skip connection (так называемый "обходной путь"). Таким образом, в генераторе используется 11 residual блоков, что позволяет создать глубокую сеть, способную на сложные преобразования стиля.

### InstanceNorm
BatchNorm нормализует по всему батчу изображений, а InstanceNorm нормализует каждое изображение отдельно. Стиль - это характеристика конкретного изображения, а не батча, поэтому в данном проекте выбран InstanceNorm для сохранения особенностей каждого изображения.

## Гиперпараметры

```
Epoch: 55
Batch size: 6
Learning rate: 0.0002
Optimizer: Adam 
Lambda cycle: 10.0
Lambda identity: 0.5
Image pool size: 50
Residual blocks: 11
Save interval: 10
```

## Обучение

```python
# Инициализация модели
model = CycleGAN(device)

# Обучение
train_cyclegan(model, dataloader, num_epochs, save_interval)
```

**Время обучения (Kaggle GPU):** ~4.5 минуты на одну эпоху (всего 55 эпох) 


## Генерация изображений

**Быстрая визуализация результатов после обучения**
!!!Картиники 5

**Генерация всех изображений для submission**

```python
#Обработка батчами для оптимизации GPU памяти.
generated_count, zip_name = generate_monet_images(
    model=model,
    photo_dir=photo_dir,
    zip_name='images.zip',
    device=device,
    batch_size=8
)
```
**Визуализация результатов из случайно отобранных созданных изображений**
!!!Картиники конечные

## Результаты

Модель переносит характерные черты стиля Моне и сохраняет checkpoint каждые 10 эпох в формате `.pt`, включая:
- Веса генераторов
- Веса дискриминаторов
- Состояния оптимизаторов
- Номер эпохи

В соревновании Kaggle "I'm Something of a Painter Myself" данная реализация CycleGAN входит в топ 15%  решений. 

## Дополнительные исследования

Так как в условиях участия в соревновании было указано, что время обучения модели и работа всего ноутбука должна ограничиваться 5 часами, модель была обучена всего на 55 эпохах (`cyclegan_monet_kaggle.ipynb`). В рамках эксперимента, вне соревнования, модель была также обучена на 155 эпохах с логгированием метрик в ClearML.(`cyclegan_monet_155_epoch_with_clearml.ipynb`)

**Графики потерь**

графики

**Визуализация результатов после обучения**

картинки

**Результаты**

Визуально модель, обученная на 155 эпохах, переносит характерные черты стиля Моне лучше, чем обученная на 55.



